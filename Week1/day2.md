### 1. Paperswithcodeì—ì„œ NLU sub task ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì—¬ ë³¸ì¸ ë¸”ë¡œê·¸ì— ì •ë¦¬í•´ë³´ì„¸ìš”. ì•„ë˜ 3ê°€ì§€ í•­ëª©ì— ëŒ€í•´ì„œ ì •ë¦¬í•˜ì„¸ìš”. (ê° í•­ëª© ê³ ë ¤ ì‚¬í•­ ì°¸ê³ )

> ğŸ”—Paperswithcode([https://paperswithcode.com/task/natural-language-understanding](https://paperswithcode.com/task/natural-language-understanding))
> 

### NLU ë€?

> ****Natural Language Understandingìœ¼ë¡œ í…ìŠ¤íŠ¸ ë¶„ë¥˜, ìì—°ì–´ ì¶”ë¡  ë° ì´ì•¼ê¸° ì´í•´ì™€ ê°™ì€ ë‹¤ì–‘í•œ ì‘ì—…ì„ í¬í•¨í•˜ëŠ” ìì—°ì–´ ì²˜ë¦¬ì˜ ì¤‘ìš”í•œ ë¶„ì•¼****
> 

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/1124610c-cef6-47f4-9215-91e5a98aa3c0/Untitled.jpeg)

ì¶œì²˜: [https://www.kakaobrain.com/blog/118](https://www.kakaobrain.com/blog/118)

NLUì˜ í™œìš© ì˜ˆì‹œ(Sub-tasks of NLU)

- Simple profanity filters(ë¹„ì†ì–´ë¥¼ ì œê±°í•˜ëŠ” í•„í„° ì—­í• )
- Sentiment detection(ê°ì •ë¶„ì„)
- Topic classification(ì£¼ì œ ë¶„ë¥˜)
- Entity detection(ê°œì²´ëª… ê²€ì¶œ

---

 1) ****Sentiment Analysis****

**- ë¬¸ì œ ì •ì˜**

- ê°ì •ë¶„ì„ì€ ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì˜ ê·¹ì„±ì„ ë¶„ë¥˜í•˜ëŠ” ì‘ì—…
- ì˜ˆë¥¼ ë“¤ì–´ í…ìŠ¤íŠ¸ ê¸°ë°˜ íŠ¸ìœ—ì€ "ê¸ì •ì ", "ë¶€ì •ì " ë˜ëŠ” "ì¤‘ë¦½"ìœ¼ë¡œ ë¶„ë¥˜
- í…ìŠ¤íŠ¸ì™€ í•¨ê»˜ ì œê³µë˜ëŠ” ë ˆì´ë¸”ì´ ì£¼ì–´ì§€ë©´ ëª¨ë¸ì´ ì˜¬ë°”ë¥¸ ê°ì •ì„ ì˜ˆì¸¡í•˜ë„ë¡ í›ˆë ¨
- ë¨¸ì‹  ëŸ¬ë‹ ì ‘ê·¼ ë°©ì‹, ì–´íœ˜ ê¸°ë°˜ ì ‘ê·¼ ë°©ì‹, ì‹¬ì§€ì–´ í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹ìœ¼ë¡œ ë¶„ë¥˜
- ê°ì • ë¶„ì„ ì—°êµ¬ì˜ ì¼ë¶€ í•˜ìœ„ ë²”ì£¼ì—ëŠ” ë‹¤ì¤‘ ëª¨ë“œ ê°ì • ë¶„ì„, ì¸¡ë©´ ê¸°ë°˜ ê°ì • ë¶„ì„, ì„¸ë¶„í™”ëœ ì˜ê²¬ ë¶„ì„, ì–¸ì–´ë³„ ê°ì • ë¶„ì„ì´ í¬í•¨
- ê°ì„± ë¶„ì„ ì‹œìŠ¤í…œì„ í‰ê°€í•˜ê¸° ìœ„í•´ SST, GLUE ë° IMDB ì˜í™” ë¦¬ë·°ì™€ ê°™ì€ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° ì„¸íŠ¸ê°€ ì‚¬ìš©

**- ë°ì´í„° ì†Œê°œ(ëŒ€í‘œì ì¸ ë°ì´í„° 1ê°œ)**

- ****MRPC (Microsoft Research Paraphrase Corpus)****
- newswire articlesì—ì„œ ìˆ˜ì§‘ëœ 5201ê°œì˜ ë¬¸ì¥ì´ ì˜ì—­ì¸ì§€ ì—¬ë¶€ì— ë”°ë¼ human annotatorsì— ì˜í•´ labelì´ êµ¬ì„±
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/e5b95b71-0e36-495d-a362-343c3684cbea/Untitled.jpeg)
    
- 4076ê°œì˜ ë¬¸ì¥ ì¤‘ 2753ê°œê°€ ì˜ì—­ì¸ training set ìœ¼ë¡œêµ¬ì„±
- 1725ê°œì˜ ë¬¸ì¥ ì¤‘ 1147ê°œê°€ ì˜ì—­ì¸ test set ìœ¼ë¡œêµ¬ì„±

Â **- SOTA(State-of-the art) ëª¨ë¸ ì†Œê°œ(ëŒ€í‘œì ì¸ ëª¨ë¸ ìµœì†Œ 2ê°œ ì´ìƒ) ë° 
   ë…¼ë¬¸ì˜ ìš”ì•½ì—ì„œ ì£¼ìš” í‚¤ì›Œë“œ**

$$
Accuracy = {TN+TP \over TN+TP+FN+FP}

$$

True Positive(TP) : ì‹¤ì œ Trueì¸ ì •ë‹µì´ Trueë¼ê³  ì˜ˆì¸¡(ì •ë‹µ)
False Positive(FP) : ì‹¤ì œ False ì¸ ì •ë‹µì´ Trueë¼ê³  ì˜ˆì¸¡(ì˜¤ë‹µ)
False Negative(FN) : ì‹¤ì œ Trueì¸ ì •ë‹µì´ False ë¼ê³  ì˜ˆì¸¡(ì˜¤ë‹µ)
True Negative(TN) : ì‹¤ì œ False ì¸ ì •ë‹µì´ False ë¼ê³  ì˜ˆì¸¡(ì •ë‹µ)

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/3a432dee-d27c-4b03-866d-ad14284c74dd/Untitled.png)

---

# 1st Model

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/7b795df1-a42b-4ad9-b1fa-0421457a2901/Untitled.png)

- ****[SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization](https://paperswithcode.com/paper/smart-robust-and-efficient-fine-tuning-for)****
- Accuracy : 93.7%
- ë…¼ë¬¸ ìš”ì•½
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/49e4907f-2274-45c8-ae4d-d3cdf619c849/Untitled.png)
    
- ê¸°ì¡´ì˜ ëª¨ë¸ì€ first pre-trained on a large text corpusì—ì„œ fine-tuned on downstream tasks ì‘ì—…ì„ í•˜ê²Œë˜ë©´ ì¢…ì¢… ë°ì´í„°ì˜ ê³¼ì í•©ì´ ë°œìƒí•˜ê±°ë‚˜ pre-trainedëœ ëª¨ë¸ì— ëŒ€í•œ ì§€ì‹ì„ ìŠì–´ë²„ë¦¬ëŠ” ê²½ìš°ê°€ ë°œìƒí•˜ëŠ”ê²ƒì„ í™•ì¸í•˜ì˜€ê³  ê·¸ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•œ new computational frameworkë¥¼ ì‹œí–‰
- Bregman proximal point optimizationì€ trust-region methodsì´ë©° ì§€ì‹ì„ ë§ê°í•˜ëŠ” ê²ƒì„ ì˜ˆë°©
- ì£¼ìš” í‚¤ì›Œë“œ
    - Smoothness-inducing regularization, which effectively manages the capacity of the model
    - Bregman proximal point optimization

# 2nd Model

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/36ca4aa0-a570-4e31-9cf1-cf732874afd1/Untitled.png)

- ****[Synthesizer: Rethinking Self-Attention in Transformer Models](https://paperswithcode.com/paper/synthesizer-rethinking-self-attention-in)****
- Accuracy : 91.2%
- ë…¼ë¬¸ ìš”ì•½
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/8cddedaa-cb27-42cd-9161-4db81a3ab80e/Untitled.png)
    
- product-based self-attention mechanismì´ Transformer ëª¨ë¸ì˜ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì¤‘ìš”ì„±ê³¼ ê¸°ì—¬ë„ë¥¼ ì¡°ì‚¬í•˜ì˜€ê³  ë§¤ìš° ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥ì„ ë‹¬ì„±
- ì£¼ìš” í‚¤ì›Œë“œ
    - self-attention
    - Random Synthesizer
    - 60% faster
    - simple factorized Synthesizers
