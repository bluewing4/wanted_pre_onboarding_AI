### 1. ë³¸ì¸ì´ ë³¸ ê°•ì˜ë¥¼ ìˆ˜ê°•í•˜ëŠ” ëª©ì ì— ëŒ€í•´ì„œ ììœ ë¡­ê²Œ ì ì–´ë³´ì„¸ìš”.

AI ë° DATA ë¶„ì•¼ì—ì„œ ì¢€ ë” ì„¸ë°€í•œ ê³¼ì •ì„ í†µí•˜ì—¬ ê¸°ìˆ ì ì¸ ê²½í—˜ì„ í†µí•˜ì—¬ í•œë‹¨ê³„ ë°œì „í•  ìˆ˜ìˆë„ë¡ ë…¸ë ¥í•˜ê² ìŠµë‹ˆë‹¤.

### 2. Paperswithcodeì—ì„œ NLP sub task ì¤‘ì— 2ê°œë¥¼ ì„ íƒí•˜ì—¬ ë³¸ì¸ ë¸”ë¡œê·¸ì— ì •ë¦¬í•´ë³´ì„¸ìš”. task ë³„ë¡œ ì•„ë˜ 3ê°€ì§€ í•­ëª©ì— ëŒ€í•´ì„œ ì •ë¦¬í•˜ì„¸ìš”. (ê° í•­ëª© ê³ ë ¤ ì‚¬í•­ ì°¸ê³ )

> ğŸ”—Paperswithcode([https://paperswithcode.com/area/natural-language-processing](https://paperswithcode.com/area/natural-language-processing))
> 

### NLP ë€?

> **Natural Language Processing(ìì—°ì–´ì²˜ë¦¬)ìœ¼ë¡œ í…ìŠ¤íŠ¸ì—ì„œ ì˜ë¯¸ìˆëŠ” ì •ë³´ë¥¼ ë¶„ì„, ì¶”ì¶œí•˜ê³  ì´í•´í•˜ëŠ” ì¼ë ¨ì˜ ê¸°ìˆ ì§‘í•©**
> 

![ì¶œì²˜: [https://www.kakaobrain.com/blog/118](https://www.kakaobrain.com/blog/118)](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/1415fcf9-25d0-4ce3-b27f-c8944d2dfddb/Untitled.jpeg)

ì¶œì²˜: [https://www.kakaobrain.com/blog/118](https://www.kakaobrain.com/blog/118)

NLP ì˜ í™œìš© ì˜ˆì‹œ(Sub-tasks of NLP)

- í…ìŠ¤íŠ¸ ìš”ì•½ (ex:Â Summly)
- ìë™ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ (ex:Â Wolfram Alpha)
- ëŒ€í™” ì‹œìŠ¤í…œ (ex:Â Apple Siri)
- ê¸°ê³„ ë²ˆì—­ (ex:Â Google Translate)

 1) ****Document Summarization****

**- ë¬¸ì œ ì •ì˜**

- ë¬¸ì„œì˜ ì¤‘ìš”í•œ ë‚´ìš©ì„ ìœ ì§€í•˜ë©´ì„œ ë¬¸ì„œë¥¼ ì§§ì€ í˜•ì‹ìœ¼ë¡œ ë³´ì—¬ì£¼ëŠ” ì‘ì—…
- extractive approaches(ì¶”ì¶œì‹ ì ‘ê·¼) ì™€ abstractive approaches(ì¶”ìƒì  ì ‘ê·¼)ì´ ìˆê³  ì¶”ì¶œì‹ ì ‘ê·¼ì€ ì›ë³¸ì˜ ì¼ë¶€ë¥¼ ì¶”ì¶œí•˜ì—¬ ìš”ì•½í•˜ëŠ” ë°˜ë©´ ì¶”ìƒì  ì ‘ê·¼ì€ ì›ë³¸ì— ì—†ëŠ” ìƒˆë¡œìš´ ë‹¨ì–´ë‚˜ êµ¬ë¥¼ ìƒì„±í•  ìˆ˜ ìˆë„ë¡ ë˜ì–´ìˆìŠµë‹ˆë‹¤.

**- ë°ì´í„° ì†Œê°œ(ëŒ€í‘œì ì¸ ë°ì´í„° 1ê°œ)**

- ë°ì´í„° CNN/Daily Mailì€ ê° ì›¹ì‚¬ì´íŠ¸ì˜ ë‰´ìŠ¤ê¸°ì‚¬ ìƒì„±ë˜ì—ˆê³  
286,817ê°œì˜ training set , 13,368ê°œì˜ validation set ë° 11,487ê°œì˜ test setìœ¼ë¡œ êµ¬ì„±ë˜ì—ˆìœ¼ë©°  training setëŠ” í‰ê·  29.74ê°œì˜ ë¬¸ì¥ì— 766ê°œì˜ ë‹¨ì–´ë¥¼ ê°€ì§€ê³  ìˆìœ¼ë©° ìš”ì•½ì€ 53ê°œì˜ ë‹¨ì–´ì™€ 3.72ê°œì˜ ë¬¸ì¥ìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/254c243b-ad9e-411d-a276-d19f69dadef6/Untitled.png)

Â **- SOTA(State-of-the art) ëª¨ë¸ ì†Œê°œ(ëŒ€í‘œì ì¸ ëª¨ë¸ 1ê°œ)**

- [All NLP Tasks Are Generation Tasks: A General Pretraining Framework](https://paperswithcode.com/paper/all-nlp-tasks-are-generation-tasks-a-general)

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/bac8c67b-5707-4009-bfa9-b52c4f68d7fc/Untitled.png)

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/68b7f557-5cde-4e41-b702-dfab571cb4a7/Untitled.png)

- GLM-XXLargeëª¨ë¸ë¡œ ë…¼ë¬¸ì˜ ë‚´ìš©ìœ¼ë¡œ GLMì€ ë™ì¼í•œ ì–‘ì˜ pre-training dataë¡œ SuperGLUE natural language understanding benchmarkì—ì„œ BERTë¥¼ í›¨ì”¬ ëŠ¥ê°€í•œë‹¤ê³  ë‚˜ì™€ìˆìŠµë‹ˆë‹¤.
- ì£¼ìš” í‚¤ì›Œë“œëŠ” pretraining framework GLM (General Language Model), ERT-like models on classification due to improved pretrain-finetune, same amount of pre-training data

 **2) Text Classification**

**- ë¬¸ì œ ì •ì˜**

- ë¬¸ì¥ì´ë‚˜ ë¬¸ì„œë¥¼ ì ì ˆí•œ ë²”ì£¼ë¡œ ì§€ì •í•˜ëŠ” ì‘ì—…
- ë²”ì£¼ëŠ” ì„ íƒí•œ ë°ì´í„° ì„¸íŠ¸ì— ë”°ë¼ ë‹¤ë¥´ë©° ì£¼ì œ ë²”ìœ„ê°€ ë‹¤ì–‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**- ë°ì´í„° ì†Œê°œ(ëŒ€í‘œì ì¸ ë°ì´í„° 1ê°œ)**

- AG News(AGì˜ News Corpus)ëŠ” 4ê°€ì§€ì˜ ê°€ì¥ í° í´ë˜ìŠ¤(â€œWorldâ€, â€œSportsâ€, â€œBusinessâ€, â€œSci/Techâ€)ë¡œ êµ¬ì„±ë˜ì–´ìˆê³  ê° í´ë˜ìŠ¤ë‹¹ 30,000ê°œì˜ training set ë° 1,900ê°œì˜ test setì…ë‹ˆë‹¤.

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/beb546c5-9d0f-44fb-996b-03d83e43922e/Untitled.jpeg)

Â **- SOTA(State-of-the art) ëª¨ë¸ ì†Œê°œ(ëŒ€í‘œì ì¸ ëª¨ë¸ 1ê°œ)**

- ****[XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://paperswithcode.com/paper/xlnet-generalized-autoregressive-pretraining)****
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/1d869585-6aa8-4274-b72c-1f3313ab5764/Untitled.png)
    
- 
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/53d7e8d6-400b-45e6-8479-176bc3e778d9/Untitled.png)
    
- **XLNet**ëª¨ë¸ë¡œ autoregressive pretraining ë°©ë²•ìœ¼ë¡œ êµ¬í˜„ë˜ì—ˆìœ¼ë©° ì´ëŠ” BERTì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê³  ì§ˆë¬¸ ë‹µë³€, ìì—°ì–´ ì¶”ë¡ , ê°ì • ë¶„ì„ ë° ë¬¸ì„œ ìˆœìœ„ë¥¼ í¬í•¨í•˜ì—¬ ì¢…ì¢… í° ì°¨ì´ë¡œ 20ê°€ì§€ ì‘ì—…ì—ì„œ ë³´ë‹¤ ë›°ì–´ë‚œ ê²°ê³¼ë¥¼ ë³´ì˜€ìŠµë‹ˆë‹¤.
- ì£¼ìš” í‚¤ì›Œë“œëŠ” autoregressive pretraining, XLNet outperforms BERT on 20 tasks, natural language inference, sentiment analysis, and document ranking
