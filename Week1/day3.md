ë³¸íŒŒì¼ : https://second-chanter-c94.notion.site/3-NLG-7905961939344a2aa4e8028816231d20
### 1. Paperswithcodeì—ì„œ NLG extractive summarization taskì— ëŒ€í•´ì„œ ë³¸ì¸ ë¸”ë¡œê·¸ì— ì •ë¦¬í•´ë³´ì„¸ìš”. ì•„ë˜ 3ê°€ì§€ í•­ëª©ì— ëŒ€í•´ì„œ ì •ë¦¬í•˜ì„¸ìš”. (ê° í•­ëª© ê³ ë ¤ ì‚¬í•­ ì°¸ê³ )

> ğŸ”—Paperswithcode([https://paperswithcode.com/area/natural-language-processing](https://paperswithcode.com/area/natural-language-processing))
> 

### NLG ë€?

> ****Natural Language Generationìœ¼ë¡œ ì»´í“¨í„°ê°€ Natural Languageë¥¼ ì¶œë ¥ìœ¼ë¡œ ìƒì„±í•˜ëŠ” í”„ë¡œì„¸ìŠ¤ë¡œ ì •ì˜ê°€ ë˜ë©° Natural Languageë¥¼ í•©ë¦¬ì ì¸ ë°©ì‹ìœ¼ë¡œ ì–´ë–»ê²Œ ìƒì„±í•  ì§€ë¥¼ ì»´í“¨í„°ì—ê²Œ ê°€ë¥´ì¹˜ëŠ” ì˜ì—­****
> 

ìì—°ì–´ ìƒì„±ì€ êµ¬ì¡°í™” ëœ ë°ì´í„°ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë§Œë“¤ì–´ ë‚´ëŠ” ê³¼ì •

NLP(ìì—°ì–´ ì²˜ë¦¬) = NLU(ìì—°ì–´ ì´í•´) + NLG(ìì—°ì–´ ìƒì„±)

![ì¶œì²˜: [https://www.kakaobrain.com/blog/118](https://www.kakaobrain.com/blog/118)](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/81d809d9-d2b0-4c43-b8e5-a047b6737f7e/Untitled.jpeg)

ì¶œì²˜: [https://www.kakaobrain.com/blog/118](https://www.kakaobrain.com/blog/118)

NLGì˜ í™œìš© ì˜ˆì‹œ(Sub-tasks of NLG)

- Machine Translation
- Summarization
- Dialogue : task-oriented system, open-domain system(social dialogue)
- Creative writing : storytelling, poetry-generation
- Freeform Question Answering
- Image captioning

---

 1) **Extractive Summarization**

**- ë¬¸ì œ ì •ì˜**

- ì£¼ì–´ì§„ ë¬¸ì„œì—ì„œ ë¬¸ì„œì˜ ìš”ì•½ì„ ê°€ì¥ ì˜ ë‚˜íƒ€ë‚´ëŠ” ë‹¨ì–´ ë˜ëŠ” ë¬¸ì¥ì˜ í•˜ìœ„ ì§‘í•©

**- ë°ì´í„° ì†Œê°œ(ëŒ€í‘œì ì¸ ë°ì´í„° 1ê°œ)**

****CNN/Daily Mail****

- ë°ì´í„° CNN/Daily Mailì€ ê° ì›¹ì‚¬ì´íŠ¸ì˜ ë‰´ìŠ¤ê¸°ì‚¬ ìƒì„±ë˜ì—ˆê³  
286,817ê°œì˜ training set , 13,368ê°œì˜ validation set ë° 11,487ê°œì˜ test setìœ¼ë¡œ êµ¬ì„±ë˜ì—ˆìœ¼ë©°  training setëŠ” í‰ê·  29.74ê°œì˜ ë¬¸ì¥ì— 766ê°œì˜ ë‹¨ì–´ë¥¼ ê°€ì§€ê³  ìˆìœ¼ë©° ìš”ì•½ì€ 53ê°œì˜ ë‹¨ì–´ì™€ 3.72ê°œì˜ ë¬¸ì¥ìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/254c243b-ad9e-411d-a276-d19f69dadef6/Untitled.png)

**- í‰ê°€ì§€í‘œ**

**ROUGE**

**ROUGE(Recall-Oriented Understudy for Gisting Evaluation)**ë€ text summarization, machine translationê³¼ ê°™ì€ generation taskë¥¼ í‰ê°€í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë˜ëŠ” ëŒ€í‘œì ì¸ Metric

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/91dac035-a717-4773-afaa-b1bb2b25711c/Untitled.png)

- **ROUGE-N** :  unigram, bigram, trigram ë“± ë¬¸ì¥ ê°„Â **ì¤‘ë³µë˜ëŠ” n-gram** ì„ ë¹„êµí•˜ëŠ” ì§€í‘œ
- ex) ROUGE-1 : unigram
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/21fc749d-f1e1-427a-b630-b0f2f7c00316/Untitled.png)
    
- ex)ROUGE-2 : bigram
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/3e417e11-9f9f-4991-9bf2-5fafec37ee9f/Untitled.png)
    
- **ROUGE-L : Longest Common Subs equenceë¡œ ê°€ì¥ ê¸´ Sequenceì˜ recallì„ êµ¬í•¨(SequenceëŠ” ì´ì–´ì§€ì§€ ì•Šì•„ë„ ë¨)**
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/6ab54af0-c571-4ff2-a0cd-753edd1a0573/Untitled.png)
    
- **ROUGE-Lì˜** ì¥ì ì€Â **ROUGE-2**ì™€ ê°™ì´ ë‹¨ì–´ë“¤ì˜Â **ì—°ì†ì  ë§¤ì¹­**ì„ ìš”êµ¬í•˜ì§€ ì•Šê³ , ì–´ë–»ê²Œë“ Â **ë¬¸ìì—´**Â ë‚´ì—ì„œ ë°œìƒí•˜ëŠ” ë§¤ì¹­ì„ ì¸¡ì •í•˜ê¸° ë•Œë¬¸ì— ë³´ë‹¤ ìœ ì—°í•œ ì„±ëŠ¥ ë¹„êµê°€ ê°€ëŠ¥

                                                                                      ì¶œì²˜ : [https://supkoon.tistory.com/26](https://supkoon.tistory.com/26)

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/f461336e-4d64-410d-81ac-356e7fd4eff1/Untitled.jpeg)

Â **- SOTA(State-of-the art) ëª¨ë¸ ì†Œê°œ(ëŒ€í‘œì ì¸ ëª¨ë¸ ìµœì†Œ 2ê°œ ì´ìƒ) ë° 
   ë…¼ë¬¸ì˜ ìš”ì•½ì—ì„œ ì£¼ìš” í‚¤ì›Œë“œ**

---

- ****Neural Extractive Text Summarization with Syntactic Compression****
- ****Extractive Summarization as Text Matching****

# 1st Model

- [Lessons on Parameter Sharing across Layers in Transformers](https://paperswithcode.com/paper/lessons-on-parameter-sharing-across-layers-in)

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/0ed48bbd-4f25-4ec1-8b4e-5e5c5330a26e/Untitled.png)

- ë…¼ë¬¸ ìš”ì•½

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/1723414e-960a-4976-b63b-3c08b703c182/Untitled.png)

- summarizationì˜ ìµœê·¼ ì ‘ê·¼ë°©ì‹ì€ selection-based extraction or generation-based abstractionì˜ ë‘ê°€ì§€
- single-document summarization based on joint extraction and syntactic compression ì œì•ˆ
- ê¸°ì„±ì˜ ì••ì¶• ëª¨ë“ˆë³´ë‹¤ ì„±ëŠ¥ì´ ë›°ì–´ë‚˜ê³  generally remains grammaticalì„ ìœ ì§€
- ì£¼ìš” í‚¤ì›Œë“œ
    - joint extraction and syntactic compression
    - generally remains grammaticalì„ ìœ ì§€

# 2nd Model

- ****[Extractive Summarization as Text Matching](https://paperswithcode.com/paper/extractive-summarization-as-text-matching)****

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/0d716c01-2b14-4fe8-80cb-b1caec573184/Untitled.png)

- ë…¼ë¬¸ ìš”ì•½

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/ae1244f5-e612-4e94-a13f-fbd70fd8df0a/Untitled.png)

- ê°œë³„ì ìœ¼ë¡œ ë¬¸ì¥ì„ ì¶”ì¶œí•˜ê³  ë¬¸ì¥ ê°„ì˜ ê´€ê³„ë¥¼ ëª¨ë¸ë§ í•˜ëŠ” ëŒ€ì‹  ì¶”ì¶œìš”ì•½ì‘ì—…ì„ ì˜ë¯¸ì í…ìŠ¤íŠ¸ë§¤ì¹­(semantic text matching)ìœ¼ë¡œ ë³€ê²½
- ë‹¨ìˆœí•œ í˜•íƒœì˜ ë§¤ì¹­ ëª¨ë¸ë¡œ í”„ë ˆì„ì›Œí¬ë¥¼ ì¸ìŠ¤í„´ìŠ¤í™” í•˜ë”ë¼ë„ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì¤Œ
- ì£¼ìš” í‚¤ì›Œë“œ
    - semantic text matching
